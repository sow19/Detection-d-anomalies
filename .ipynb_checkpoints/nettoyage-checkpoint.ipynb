{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b3788f0d-223f-4209-980d-cd1faa288b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mariama/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mariama/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mariama/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mariama/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "85858e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronym_definitions = {\n",
    "    \"COPD\": \"Chronic Obstructive Pulmonary Disease\",\n",
    "    \"CT\": \"Computed Tomography\",\n",
    "    \"PPD\": \"Purified Protein Derivative\",\n",
    "    \"PA\": \"Posterior-Anterior\",\n",
    "    \"SVC\": \"Superior Vena Cava\",\n",
    "    \"CABG\": \"Coronary Artery Bypass Graft\",\n",
    "    \"CHF\": \"Congestive Heart Failure\",\n",
    "    \"HIV\": \"Human Immunodeficiency Virus\",\n",
    "    \"MRI\": \"Magnetic Resonance Imaging\",\n",
    "    \"EKG\": \"Electrocardiogram\",\n",
    "    \"AP\": \"Anteroposterior\",\n",
    "    \"PICC\": \"Peripherally Inserted Central Catheter\",\n",
    "    \"TB\": \"Tuberculosis\",\n",
    "    \"MVA\": \"Motor Vehicle Accident\",\n",
    "    \"CHEST\": \"Thoracic Cavity (context-dependent)\",\n",
    "    \"SOB\": \"Shortness of Breath\",\n",
    "    \"MVC\": \"Motor Vehicle Collision\",\n",
    "    \"IJ\": \"Internal Jugular\",\n",
    "    \"ICD\": \"Implantable Cardioverter Defibrillator\",\n",
    "    \"BMT\": \"Bone Marrow Transplant\",\n",
    "    \"DYSPNEA\": \"Difficulty Breathing\",\n",
    "    \"LAB\": \"Laboratory\",\n",
    "    \"PRE\": \"Preoperative\",\n",
    "    \"OP\": \"Operative Procedure\",\n",
    "    \"CXR\": \"Chest X-Ray\",\n",
    "    \"CP\": \"Chest Pain\",\n",
    "    \"KUB\": \"Kidneys, Ureters, and Bladder\",\n",
    "    \"IV\": \"Intravenous\",\n",
    "    \"AML\": \"Acute Myeloid Leukemia\",\n",
    "    \"ECF\": \"Extracellular Fluid\",\n",
    "    \"LUNG\": \"Lung (context-dependent)\",\n",
    "    \"NECK\": \"Neck (context-dependent)\",\n",
    "    \"DKA\": \"Diabetic Ketoacidosis\",\n",
    "    \"IVC\": \"Inferior Vena Cava\",\n",
    "    \"AICD\": \"Automatic Implantable Cardioverter Defibrillator\",\n",
    "    \"NG\": \"Nasogastric\",\n",
    "    \"VP\": \"Ventriculoperitoneal\",\n",
    "    \"PX\": \"Physical Examination\",\n",
    "    \"PT\": \"Physical Therapy\",\n",
    "    \"CA\": \"Cancer\",\n",
    "    \"KV\": \"Kilovolt\",\n",
    "    \"DVT\": \"Deep Vein Thrombosis\",\n",
    "    \"CLL\": \"Chronic Lymphocytic Leukemia\",\n",
    "    \"HAS\": \"Hypertension Associated Symptoms (context-dependent)\",\n",
    "    \"TNF\": \"Tumor Necrosis Factor\",\n",
    "    \"BR\": \"Breast\",\n",
    "    \"SBP\": \"Systolic Blood Pressure\",\n",
    "    \"WBC\": \"White Blood Cell\",\n",
    "    \"ATV\": \"All-Terrain Vehicle (context-dependent)\",\n",
    "    \"SDH\": \"Subdural Hematoma\",\n",
    "    \"CCK\": \"Cholecystokinin\",\n",
    "    \"XRAY\": \"Radiograph\",\n",
    "    \"HX\": \"History\",\n",
    "    \"APNEA\": \"Temporary Cessation of Breathing\",\n",
    "    \"TKA\": \"Total Knee Arthroplasty\",\n",
    "    \"PTX\": \"Pneumothorax\",\n",
    "    \"HF\": \"Heart Failure\",\n",
    "    \"CIWA\": \"Clinical Institute Withdrawal Assessment\",\n",
    "    \"DORV\": \"Double Outlet Right Ventricle\",\n",
    "    \"CVA\": \"Cerebrovascular Accident\",\n",
    "    \"BNP\": \"B-Type Natriuretic Peptide\",\n",
    "    \"PNA\": \"Pneumonia\",\n",
    "    \"POS\": \"Positive\",\n",
    "    \"DX\": \"Diagnosis\",\n",
    "    \"AIDS\": \"Acquired Immunodeficiency Syndrome\",\n",
    "    \"RLL\": \"Right Lower Lobe\",\n",
    "    \"RT\": \"Radiation Therapy\",\n",
    "    \"RUQ\": \"Right Upper Quadrant\",\n",
    "    \"UIP\": \"Usual Interstitial Pneumonia\",\n",
    "    \"GYN\": \"Gynecology\",\n",
    "    \"ESR\": \"Erythrocyte Sedimentation Rate\",\n",
    "    \"AAM\": \"African American Male\",\n",
    "    \"PDA\": \"Patent Ductus Arteriosus\",\n",
    "    \"SVT\": \"Supraventricular Tachycardia\",\n",
    "    \"AFIB\": \"Atrial Fibrillation\",\n",
    "    \"POD\": \"Postoperative Day\",\n",
    "    \"VHR\": \"Ventral Hernia Repair\",\n",
    "    \"UTI\": \"Urinary Tract Infection\",\n",
    "    \"ORIF\": \"Open Reduction and Internal Fixation\",\n",
    "    \"AVN\": \"Avascular Necrosis\",\n",
    "    \"STAB\": \"Stab Wound (context-dependent)\",\n",
    "    \"SP\": \"Status Post\",\n",
    "    \"HCC\": \"Hepatocellular Carcinoma\",\n",
    "    \"SBRT\": \"Stereotactic Body Radiation Therapy\",\n",
    "    \"BCG\": \"Bacillus Calmette-Guérin\",\n",
    "    \"SX\": \"Symptoms\",\n",
    "    \"PM\": \"Post Mortem (or Pacemaker, context-dependent)\",\n",
    "    \"CTA\": \"Computed Tomography Angiography\",\n",
    "    \"AS\": \"Aortic Stenosis\",\n",
    "    \"VSD\": \"Ventricular Septal Defect\",\n",
    "    \"SM\": \"Small Molecule (context-dependent)\",\n",
    "    \"TOF\": \"Tetralogy of Fallot\",\n",
    "    \"PPM\": \"Permanent Pacemaker\",\n",
    "    \"INR\": \"International Normalized Ratio\",\n",
    "    \"Dr.\": \"doctor\"\n",
    "}\n",
    "\n",
    "def acronym_means(text):\n",
    "    new_text = []\n",
    "    for i in text.split():\n",
    "        if i.upper() in acronym_definitions:\n",
    "            new_text.append(acronym_definitions[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f8e70fc-737a-4438-a32c-d0c41ebd36e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition des stopwords et Lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cd262f34-5214-4a67-91cd-2353b323d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour convertir en minuscules\n",
    "def to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce12aa58-873d-43fe-8ad1-859c7d2450a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour supprimer la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13a7ac86-76cc-4ea4-b655-fc5a5911c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour supprimer les stopwords et appliquer la lemmatisation\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7ff6c8e5-f9af-4537-b186-9d1037e98700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_xxx(text):\n",
    "    # Ne pas supprimer les chiffres suivis de \"cm\" (par exemple, \"12 cm\")\n",
    "    text = re.sub(r'\\b(\\d+)(?=\\s*cm\\b)', r'@@\\1@@', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remettre les chiffres marqués (ceux suivis de \"cm\")\n",
    "    text = text.replace('@@', '')\n",
    "    text = text.replace(\"xxxx\", \"\").replace(\"x\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b990c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour générer des n_grame (trigrams)\n",
    "def generate_n_grams(text, min_count=1, threshold=1):\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        # Diviser le texte en tokens\n",
    "        corpus = [text.split()]\n",
    "    else:\n",
    "        raise ValueError(\"Le texte d'entrée doit être une chaîne de caractères.\")\n",
    "\n",
    "    # Modèle de bigrammes\n",
    "    bigram_model = Phrases(corpus, min_count=min_count, threshold=threshold)\n",
    "    bigrams = [bigram_model[doc] for doc in corpus]\n",
    "\n",
    "    # Modèle de trigrammes\n",
    "    trigram_model = Phrases(bigrams, min_count=min_count, threshold=threshold)\n",
    "    trigrams = [trigram_model[doc] for doc in bigrams]\n",
    "\n",
    "    # Conversion en trigrammes\n",
    "    result = []\n",
    "    for doc in trigrams:\n",
    "        for gram in doc:\n",
    "            if \"_\" in gram: \n",
    "                result.append(gram)\n",
    "            else:\n",
    "                result.append(gram)  # Garder les mots non reliés\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72f9cc9c-9f67-4ae8-bc93-6f7b787b87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction principale de prétraitement pour une colonne de texte\n",
    "def preprocess_text_column(column):\n",
    "    try:\n",
    "        column = column.astype(str)\n",
    "        column = column.apply(acronym_means)\n",
    "        column = column.apply(to_lowercase)\n",
    "        column = column.apply(remove_punctuation)\n",
    "        column = column.apply(remove_stopwords_and_lemmatize)\n",
    "        column = column.apply(remove_xxx)\n",
    "        column = column.apply(generate_n_grams)\n",
    "        return column\n",
    "    except Exception as e:\n",
    "        print(\"Erreur dans le prétraitement :\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e4514e2-6e43-4c46-9b98-c0bdf6638cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour combiner des columns\n",
    "def combine_columns(df, text_columns):\n",
    "    \"\"\"\n",
    "    Combine les colonnes spécifiées d'un DataFrame en une seule colonne nommée 'combined_text',\n",
    "    puis supprime toutes les autres colonnes.\n",
    "    Retourne un DataFrame ne contenant que la colonne combinée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Combiner les colonnes spécifiées dans une seule chaîne de texte par ligne\n",
    "        df['combined_text'] = df[text_columns].fillna('').astype(str).agg(' '.join, axis=1)\n",
    "        # Ne conserver que la colonne combinée\n",
    "        df = df[['combined_text']]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors de la combinaison des colonnes :\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6216cc63-63fe-4149-9345-ceaf73984bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef combine_dataframes(df1, df2):\\n    try:\\n        if df1.shape[1] != 1 or df2.shape[1] != 1:\\n            print(\"Erreur : Les DataFrames doivent avoir une seule colonne.\")\\n            return None\\n        df1 = df1.iloc[:, 0]\\n        df2 = df2.iloc[:, 0]\\n        combined_df = pd.concat([df1, df2], ignore_index=True)\\n        combined_df = combined_df.to_frame(\\'combined_text\\')\\n        return combined_df\\n    except Exception as e:\\n        print(\"Erreur lors de la combinaison des DataFrames :\", e)\\n        return None\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fonction pour combiner les dataFrames\n",
    "\"\"\"\n",
    "def combine_dataframes(df1, df2):\n",
    "    try:\n",
    "        if df1.shape[1] != 1 or df2.shape[1] != 1:\n",
    "            print(\"Erreur : Les DataFrames doivent avoir une seule colonne.\")\n",
    "            return None\n",
    "        df1 = df1.iloc[:, 0]\n",
    "        df2 = df2.iloc[:, 0]\n",
    "        combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "        combined_df = combined_df.to_frame('combined_text')\n",
    "        return combined_df\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors de la combinaison des DataFrames :\", e)\n",
    "        return None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5842a2df-f86e-41ad-b33e-f0187fdd827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, column_text='combined_text'):\n",
    "    \"\"\"\n",
    "    Applique le prétraitement sur une colonne spécifique du DataFrame (par défaut 'combined_text').\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Le DataFrame contenant les données à traiter.\n",
    "        column_text (str): Le nom de la colonne à traiter (par défaut 'combined_text').\n",
    "\n",
    "    Returns:\n",
    "        bool: True si le prétraitement a réussi, False sinon.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if column_text not in df.columns:\n",
    "            print(f\"Erreur : La colonne '{column_text}' est manquante dans le DataFrame.\")\n",
    "            return False\n",
    "        \n",
    "        # Appliquer le prétraitement sur la colonne spécifiée\n",
    "        df[column_text] = preprocess_text_column(df[column_text])\n",
    "        if df[column_text].isnull().all():\n",
    "            print(\"Erreur : Le prétraitement a échoué, aucune donnée valide.\")\n",
    "            return False\n",
    "\n",
    "        print(\"Prétraitement réussi.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur dans le prétraitement global : {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4735443e-d7aa-4c50-a02a-6aeaacf41d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_pipeline(csv_path, csv2_path, text_columns):\n",
    "    \"\"\"\n",
    "    Exécute le pipeline complet de combinaison des fichiers, prétraitement des données,\n",
    "    et renvoi du DataFrame traité ou d'un message d'erreur.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Chemin vers le fichier CSV.\n",
    "        excel_path (str): Chemin vers le fichier Excel.\n",
    "        text_columns1 (list): Colonnes à combiner dans le DataFrame CSV.\n",
    "\n",
    "    Returns:\n",
    "        final_dat: Affiche le dataframe final si tout se passe bien .\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data1 = pd.read_csv(csv_path)\n",
    "        data2 = pd.read_csv(csv2_path)\n",
    "\n",
    "        # Étape 2 : Combiner les colonnes de chaque DataFrame\n",
    "        new_data1 = combine_columns(data1, text_columns)\n",
    "        if new_data1 is not None and data2 is not None:\n",
    "            # Étape 4 : Appliquer le prétraitement global sur la colonne 'combined_text'\n",
    "            success1 = preprocess_dataframe(new_data1, column_text='combined_text')\n",
    "            success2 = preprocess_dataframe(data2, column_text=\"Clinician's Notes\")\n",
    "            if success1&success2:\n",
    "                print(\"Le pipeline complet a réussi.\")\n",
    "                return new_data1, data2\n",
    "            else:\n",
    "                print(\"Le prétraitement a échoué.\")\n",
    "        else:\n",
    "            print(\"Erreur les dataset sont vides\")\n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite dans le pipeline : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8e7eee2-7058-4360-b7b7-ed545e8dbf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373930/1455994777.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column_text] = preprocess_text_column(df[column_text])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitement réussi.\n",
      "Prétraitement réussi.\n",
      "Le pipeline complet a réussi.\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"final_dataset.csv\"\n",
    "csv2_path = \"Radiologists Report.csv\"\n",
    "text_columns = ['abstract_FINDINGS', 'abstract_IMPRESSION']\n",
    "final_data1, final_data2 = execute_pipeline(csv_path, csv2_path, text_columns)\n",
    "final_data1.to_csv('clean_data.csv', index=False)\n",
    "final_data2.to_csv('clean_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "162e0bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"/export/spacy-libs/fr_core_news_sm/fr_core_news_sm-3.1.0\")\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e6d7de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Densité des points dans les dimensions de l'espace des plongements lexicaux\n",
    "from sklearn.neighbors import KernelDensity\n",
    "def estimate_density(x_vals, y_vals, bandwidth=0.1):\n",
    "    \"\"\"\n",
    "    Estime la densité des points à l'aide du Kernel Density Estimation (KDE) dans le but de pouvoir donner un zoom \n",
    "    de la visualisation des points.\n",
    "    :param x_vals: Liste des valeurs de la dimension 1\n",
    "    :param y_vals: Liste des valeurs de la dimension 2\n",
    "    :param bandwidth: Paramètre du noyau KDE, plus petit = plus précis, plus grand = plus lisse\n",
    "    :return: Coordonnées de la zone de densité élevée\n",
    "    \"\"\"\n",
    "    # Empile les valeurs x et y pour l'estimation KDE\n",
    "    points = np.vstack([x_vals, y_vals]).T\n",
    "\n",
    "    # KDE pour estimer la densité\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth)\n",
    "    kde.fit(points)\n",
    "    \n",
    "    # Calcul des densités pour une grille de points\n",
    "    x_grid = np.linspace(min(x_vals), max(x_vals), 100)\n",
    "    y_grid = np.linspace(min(y_vals), max(y_vals), 100)\n",
    "    X, Y = np.meshgrid(x_grid, y_grid)\n",
    "    grid_points = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    densities = np.exp(kde.score_samples(grid_points))\n",
    "\n",
    "    # Trouver la région la plus dense\n",
    "    max_density_index = np.argmax(densities)\n",
    "    max_density_point = grid_points[max_density_index]\n",
    "\n",
    "    return max_density_point, densities.reshape(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "46994ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation_with_zoom(x_vals, y_vals, labels, zoom=True, bandwidth=0.1):\n",
    "    \"\"\"\n",
    "    Affiche un nuage de points avec un zoom automatique sur la zone la plus dense.\n",
    "    :param x_vals: Liste des valeurs de la dimension 1\n",
    "    :param y_vals: Liste des valeurs de la dimension 2\n",
    "    :param labels: Liste des labels pour chaque point\n",
    "    :param zoom: Booléen indiquant si un zoom doit être effectué (par défaut True)\n",
    "    :param bandwidth: Paramètre de l'estimation de la densité\n",
    "    \"\"\"\n",
    "    # Affichage du premier graphique sans zoom\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.scatter(x_vals, y_vals, alpha=0.7, s=50)\n",
    "    plt.grid(visible=True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "    for i in range(len(labels)):\n",
    "        plt.annotate(\n",
    "            labels[i],\n",
    "            (x_vals[i], y_vals[i]),\n",
    "            fontsize=9,\n",
    "            alpha=0.8,\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(5, 5)\n",
    "        )\n",
    "    plt.title(\"Nuage de points des plongements lexicaux - Sans zoom\", fontsize=16)\n",
    "    plt.xlabel(\"Dimension 1\", fontsize=14)\n",
    "    plt.ylabel(\"Dimension 2\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    # Estimation de la densité\n",
    "    max_density_point, densities = estimate_density(x_vals, y_vals, bandwidth)\n",
    "\n",
    "    # Définir les limites du zoom autour de la zone de densité maximale\n",
    "    zoom_limits = {\n",
    "        'x': (max_density_point[0] - 1, max_density_point[0] + 1),\n",
    "        'y': (max_density_point[1] - 1, max_density_point[1] + 1)\n",
    "    }\n",
    "\n",
    "    # Affichage avec zoom automatique basé sur la densité\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.scatter(x_vals, y_vals, alpha=0.7, s=50)\n",
    "\n",
    "    # Si le zoom est activé, ajuster les limites\n",
    "    if zoom:\n",
    "        plt.xlim(zoom_limits['x'])  # Plage des valeurs pour la dimension 1\n",
    "        plt.ylim(zoom_limits['y'])  # Plage des valeurs pour la dimension 2\n",
    "\n",
    "    plt.grid(visible=True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "    for i in range(len(labels)):\n",
    "        plt.annotate(\n",
    "            labels[i],\n",
    "            (x_vals[i], y_vals[i]),\n",
    "            fontsize=9,\n",
    "            alpha=0.8,\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(5, 5)\n",
    "        )\n",
    "    plt.title(\"Nuage de points des plongements lexicaux - Avec zoom sur densité\", fontsize=16)\n",
    "    plt.xlabel(\"Dimension 1\", fontsize=14)\n",
    "    plt.ylabel(\"Dimension 2\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0618935c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduce_dimensions_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_vals_pca, y_vals_pca, labels_pca \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_dimensions_pca\u001b[49m(model\u001b[38;5;241m.\u001b[39mwv, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      4\u001b[0m visualisation_with_zoom(x_vals_pca, y_vals_pca, labels_pca, zoom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bandwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reduce_dimensions_pca' is not defined"
     ]
    }
   ],
   "source": [
    "x_vals_pca, y_vals_pca, labels_pca = reduce_dimensions_pca(model.wv, 1000)\n",
    "\n",
    "\n",
    "visualisation_with_zoom(x_vals_pca, y_vals_pca, labels_pca, zoom=True, bandwidth=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_words(column, top_n=5):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    x = vectorizer.fit_transform(column)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords_per_text = []\n",
    "    for doc_idx in range(x.shape[0]):\n",
    "        # Récupérer les scores TF-IDF pour le texte courant\n",
    "        tfidf_scores = x[doc_idx].toarray()[0]\n",
    "\n",
    "        # Associer chaque mot avec son score TF-IDF\n",
    "        word_scores = [(feature_names[i], tfidf_scores[i]) for i in range(len(feature_names))]\n",
    "\n",
    "        # Trier les mots par score décroissant\n",
    "        sorted_words = sorted(word_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Récupérer les top N mots-clés\n",
    "        keywords = [word for word, score in sorted_words[:top_n]]\n",
    "        keywords_per_text.append(keywords)\n",
    "\n",
    "    return keywords_per_text\n",
    "    \n",
    "csv_path = \"clean_data.csv\"\n",
    "csv_path2 = \"clean_data2.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "data2 = pd.read_csv(csv_path2)\n",
    "column = preprocess_text_column(data[\"combined_text\"])\n",
    "column2 = preprocess_text_column(data2[\"Clinician's Notes\"])\n",
    "data1_kw = key_words(column)\n",
    "data2_kw = key_words(column2)\n",
    "\n",
    "data2_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9147d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def contamination_function(data1, data2, data1_kw, data2_kw, contamination_rate=0.3):\n",
    "    contaminated_texts = []\n",
    "    contamination_log = []\n",
    "\n",
    "    #pour mesurer la similarité entre les mots-clés\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_keywords = data1_kw + data2_kw\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(kw) for kw in all_keywords])\n",
    "    base_vectors = tfidf_matrix[:len(data1_kw)]\n",
    "    anomaly_vectors = tfidf_matrix[len(data1_kw):]\n",
    "    \n",
    "    for i, base_text in enumerate(data1):\n",
    "        words = base_text.split()  \n",
    "        total_words = len(words)\n",
    "        num_words_to_contaminate = max(1, int(total_words * contamination_rate))  # mots à contaminer\n",
    "        # Trouver les anomalies les plus similaires (mots clés du text avec toutes les anomalies)\n",
    "        similarities = cosine_similarity(base_vectors[i], anomaly_vectors)\n",
    "        best_anomaly_idx = similarities.argmax()\n",
    "        anomaly_text = data2[best_anomaly_idx]\n",
    "        anomaly_words = anomaly_text.split()  # les mots de l'anomalie\n",
    "        # Contaminer les mots du texte de base\n",
    "        contaminated_words = words[:]\n",
    "        contaminated_positions = []\n",
    "\n",
    "        for _ in range(num_words_to_contaminate):\n",
    "            # Choisir un mot d'anomalie aléatoire\n",
    "            anomaly_word = random.choice(anomaly_words)\n",
    "            # Choisir une position aléatoire dans le texte de base\n",
    "            position = random.randint(0, total_words - 1)\n",
    "\n",
    "            # Remplacer le mot à cette position\n",
    "            contaminated_words[position] = anomaly_word\n",
    "            contaminated_positions.append((position, anomaly_word))\n",
    "\n",
    "        # Reconstituer le texte contaminé\n",
    "        contaminated_text = ' '.join(contaminated_words)\n",
    "        contaminated_texts.append(contaminated_text)\n",
    "\n",
    "        # Journaliser les contaminations\n",
    "        contamination_log.append({\n",
    "            \"base_text\": base_text,\n",
    "            \"\\ncontaminated_text\": contaminated_text,\n",
    "            \"\\ncontaminated_positions\": contaminated_positions,\n",
    "            \"\\nanomaly_text\": anomaly_text\n",
    "        })\n",
    "\n",
    "    return contaminated_texts, contamination_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_texts = column\n",
    "anomaly_texts = column2\n",
    "base_keywords = data1_kw\n",
    "anomaly_keywords = data2_kw\n",
    "\n",
    "contaminated_texts, log = contamination_function(base_texts, anomaly_texts, base_keywords, anomaly_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_texts = column\n",
    "anomaly_texts = column2\n",
    "base_keywords = data1_kw\n",
    "anomaly_keywords = data2_kw\n",
    "\n",
    "contaminated_texts, log = contamination_function(base_texts, anomaly_texts, base_keywords, anomaly_keywords)\n",
    "for original, contaminated in zip(base_texts, contaminated_texts):\n",
    "    print(\"Texte original :\")\n",
    "    print(original)\n",
    "    print(\"\\nTexte contaminé :\")\n",
    "    print(contaminated)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "print(\"Journal des anomalies :\")\n",
    "for entry in log:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34048e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f407b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac863fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7959c7",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
